"""
Contrastive Hebbian Learning (CHL) with Spectral Normalization

CHL uses positive/negative phase dynamics similar to EqProp,
but with explicit Hebbian update rules: ΔW ∝ (y+ @ y+.T - y- @ y-.T)

Key advantage: Pure local learning (no backprop required)
Key challenge: Phase drift and weight explosion without spectral normalization

Reference: Movellan, J. R. (1991). Contrastive Hebbian learning in the 
continuous Hopfield model.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple
from torch.nn.utils.parametrizations import spectral_norm

from .nebc_base import NEBCBase, register_nebc


@register_nebc('chl')
class ContrastiveHebbianLearning(NEBCBase):
    """
    Modern Contrastive Hebbian Learning with spectral normalization.
    
    Uses two-phase dynamics:
    - Positive phase: Clamp output to target, relax network
    - Negative phase: Free relaxation (no clamping)
    
    Weight update: ΔW ∝ (h_pos @ h_pos.T - h_neg @ h_neg.T) / β
    """
    
    algorithm_name = "CHL"
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int = 2,
        use_spectral_norm: bool = True,
        max_steps: int = 30,
        alpha: float = 0.5,  # Relaxation rate
        beta: float = 0.1,   # Nudge strength (for learning)
    ):
        self.alpha = alpha
        self.beta = beta
        super().__init__(
            input_dim, hidden_dim, output_dim,
            num_layers, use_spectral_norm, max_steps
        )
    
    def _build_layers(self):
        """Build CHL network layers."""
        # Input projection
        self.W_in = nn.Linear(self.input_dim, self.hidden_dim)
        if self.use_spectral_norm:
            self.W_in = spectral_norm(self.W_in, n_power_iterations=5)
        
        # Symmetric connections (Hopfield-like)
        # For CHL, weights should be symmetric for energy minimization
        self.W_hidden = nn.Linear(self.hidden_dim, self.hidden_dim)
        if self.use_spectral_norm:
            self.W_hidden = spectral_norm(self.W_hidden, n_power_iterations=5)
        
        # Additional hidden layers if num_layers > 1
        self.layers = nn.ModuleList()
        for _ in range(self.num_layers - 1):
            layer = nn.Linear(self.hidden_dim, self.hidden_dim)
            if self.use_spectral_norm:
                layer = spectral_norm(layer, n_power_iterations=5)
            self.layers.append(layer)
        
        # Output layer
        self.W_out = nn.Linear(self.hidden_dim, self.output_dim)
        if self.use_spectral_norm:
            self.W_out = spectral_norm(self.W_out, n_power_iterations=5)
    
    def _relax(
        self, 
        x: torch.Tensor, 
        steps: int,
        target: Optional[torch.Tensor] = None,
        clamp_strength: float = 0.0,
    ) -> torch.Tensor:
        """
        Relax network to equilibrium.
        
        Args:
            x: Input tensor
            steps: Number of relaxation steps
            target: Target output (for positive phase)
            clamp_strength: How strongly to clamp to target (0 = free, 1 = hard clamp)
        """
        batch_size = x.size(0)
        h = torch.zeros(batch_size, self.hidden_dim, device=x.device)
        
        x_proj = self.W_in(x)
        
        for _ in range(steps):
            # Compute recurrent activation
            recurrent = self.W_hidden(h)
            
            # Add contributions from additional layers
            for layer in self.layers:
                recurrent = recurrent + layer(torch.tanh(recurrent))
            
            # Update hidden state
            h = (1 - self.alpha) * h + self.alpha * torch.tanh(x_proj + recurrent)
            
            # If clamping, nudge output toward target
            if target is not None and clamp_strength > 0:
                output = self.W_out(h)
                # Soft clamp by nudging hidden state
                error = target - output
                nudge = error @ self.W_out.weight  # Backproject error to hidden
                h = h + clamp_strength * self.beta * nudge
        
        return h
    
    def free_phase(self, x: torch.Tensor, steps: Optional[int] = None) -> torch.Tensor:
        """Negative phase: relaxation without target clamping."""
        steps = steps or self.max_steps
        return self._relax(x, steps, target=None, clamp_strength=0.0)
    
    def clamped_phase(
        self, 
        x: torch.Tensor, 
        target: torch.Tensor,
        steps: Optional[int] = None,
    ) -> torch.Tensor:
        """Positive phase: relaxation with output clamped to target."""
        steps = steps or self.max_steps
        return self._relax(x, steps, target=target, clamp_strength=1.0)
    
    def forward(
        self, 
        x: torch.Tensor, 
        steps: Optional[int] = None
    ) -> torch.Tensor:
        """Forward pass (free phase relaxation)."""
        h = self.free_phase(x, steps)
        return self.W_out(h)
    
    def contrastive_update(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        steps: Optional[int] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute contrastive Hebbian update.
        
        Returns (h_positive, h_negative) for computing weight updates.
        
        Weight update rule: ΔW ∝ (h_pos @ h_pos.T - h_neg @ h_neg.T)
        """
        steps = steps or self.max_steps
        
        # Convert labels to one-hot targets
        if y.dim() == 1:
            target = F.one_hot(y, self.output_dim).float()
        else:
            target = y
        
        # Positive phase (clamped)
        h_pos = self.clamped_phase(x, target, steps)
        
        # Negative phase (free)
        h_neg = self.free_phase(x, steps)
        
        return h_pos, h_neg
    
    def compute_hebbian_update(
        self,
        h_pos: torch.Tensor,
        h_neg: torch.Tensor,
    ) -> torch.Tensor:
        """
        Compute the Hebbian weight update.
        
        ΔW = (h_pos @ h_pos.T - h_neg @ h_neg.T) / (β * batch_size)
        """
        batch_size = h_pos.size(0)
        
        # Outer products
        pos_corr = h_pos.T @ h_pos / batch_size
        neg_corr = h_neg.T @ h_neg / batch_size
        
        delta_W = (pos_corr - neg_corr) / self.beta
        return delta_W
    
    def get_stats(self) -> Dict[str, float]:
        """Get CHL-specific statistics."""
        stats = super().get_stats()
        stats['beta'] = self.beta
        stats['alpha'] = self.alpha
        return stats


@register_nebc('chl_autoencoder')
class CHLAutoencoder(ContrastiveHebbianLearning):
    """
    CHL variant for unsupervised learning / autoencoding.
    
    Uses contrastive dynamics to learn data manifold.
    """
    
    algorithm_name = "CHL_AE"
    
    def _build_layers(self):
        """Build autoencoder architecture."""
        super()._build_layers()
        
        # Decoder mirrors encoder
        self.decoder = nn.Linear(self.hidden_dim, self.input_dim)
        if self.use_spectral_norm:
            self.decoder = spectral_norm(self.decoder, n_power_iterations=5)
    
    def reconstruct(self, x: torch.Tensor, steps: Optional[int] = None) -> torch.Tensor:
        """Encode and decode."""
        h = self.free_phase(x, steps)
        return self.decoder(h)
    
    def forward(
        self, 
        x: torch.Tensor, 
        steps: Optional[int] = None
    ) -> torch.Tensor:
        """For classification, use hidden state."""
        h = self.free_phase(x, steps)
        return self.W_out(h)
