"""
Causal Transformer EqProp for Language Modeling (Track 37)

Extends TransformerEqProp with causal masking and language modeling head
for character-level and token-level language modeling tasks.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils.parametrizations import spectral_norm
from .utils import spectral_linear

class CausalEqPropAttention(nn.Module):
    """Self-attention with causal masking for autoregressive generation."""
    
    def __init__(self, hidden_dim: int, num_heads: int = 4, use_sn: bool = True):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        self.W_q = spectral_linear(hidden_dim, hidden_dim, use_sn=use_sn)
        self.W_k = spectral_linear(hidden_dim, hidden_dim, use_sn=use_sn)
        self.W_v = spectral_linear(hidden_dim, hidden_dim, use_sn=use_sn)
        self.W_o = spectral_linear(hidden_dim, hidden_dim, use_sn=use_sn)
        
    def forward(self, h: torch.Tensor, causal_mask: torch.Tensor = None) -> torch.Tensor:
        batch_size, seq_len, _ = h.shape
        
        Q = self.W_q(h).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.W_k(h).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.W_v(h).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
        
        # Apply causal mask
        if causal_mask is not None:
            scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))
        
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)
        
        return self.W_o(out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim))


class CausalTransformerEqProp(nn.Module):
    """
    TransformerEqProp with causal masking for language modeling.
    
    Key differences from classification TransformerEqProp:
    - Causal attention mask
    - LM head instead of classification head
    - Outputs logits for full sequence (not just final token)
    """
    
    def __init__(
        self,
        vocab_size: int,
        hidden_dim: int = 256,
        num_layers: int = 4,
        num_heads: int = 4,
        max_seq_len: int = 512,
        eq_steps: int = 20,
        alpha: float = 0.5
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.eq_steps = eq_steps
        self.alpha = alpha
        self.max_seq_len = max_seq_len
        
        # Embeddings
        self.token_emb = nn.Embedding(vocab_size, hidden_dim)
        self.pos_emb = nn.Embedding(max_seq_len, hidden_dim)
        
        # Transformer layers
        self.attentions = nn.ModuleList([
            CausalEqPropAttention(hidden_dim, num_heads) for _ in range(num_layers)
        ])
        
        self.ffns = nn.ModuleList([
            nn.Sequential(
                spectral_linear(hidden_dim, hidden_dim * 2),
                nn.ReLU(),
                spectral_linear(hidden_dim * 2, hidden_dim)
            ) for _ in range(num_layers)
        ])
        
        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])
        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])
        
        # Language modeling head
        self.lm_head = nn.Linear(hidden_dim, vocab_size)
        
        # Create causal mask (register as buffer so it moves with model)
        self.register_buffer('causal_mask', None)
        self._create_causal_mask(max_seq_len)
    
    def _create_causal_mask(self, seq_len):
        """Create upper triangular causal mask."""
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        self.register_buffer('causal_mask', mask)
    
    def forward(self, x: torch.Tensor, steps: int = None) -> torch.Tensor:
        """
        Forward pass with equilibrium settling.
        
        Args:
            x: Input token IDs [batch, seq_len]
            steps: Number of equilibrium iterations
        
        Returns:
            Logits [batch, seq_len, vocab_size]
        """
        steps = steps or self.eq_steps
        batch_size, seq_len = x.shape
        
        # Token + position embeddings
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        x_emb = self.token_emb(x) + self.pos_emb(positions)
        
        # Get causal mask for this sequence length
        causal_mask = self.causal_mask[:seq_len, :seq_len] if self.causal_mask is not None else None
        
        # Initialize hidden state
        h = torch.zeros_like(x_emb)
        
        # Equilibrium settling
        for _ in range(steps):
            for i in range(self.num_layers):
                # Self-attention with causal mask
                h_norm = self.norms1[i](h)
                h = h + self.attentions[i](h_norm, causal_mask=causal_mask)
                
                # FFN
                h_norm = self.norms2[i](h)
                ffn_out = self.ffns[i](h_norm)
                
                # Update with input embedding and FFN
                h_target = h + ffn_out + x_emb
                h = (1 - self.alpha) * h + self.alpha * torch.tanh(h_target)
        
        # LM head: predict next token for each position
        logits = self.lm_head(h)  # [batch, seq_len, vocab_size]
        
        return logits
    
    def generate(self, prompt: torch.Tensor, max_new_tokens: int = 100, temperature: float = 1.0):
        """
        Autoregressive generation.
        
        Args:
            prompt: Starting tokens [1, prompt_len]
            max_new_tokens: Number of tokens to generate
            temperature: Sampling temperature
        
        Returns:
            Generated sequence [1, prompt_len + max_new_tokens]
        """
        self.eval()
        generated = prompt.clone()
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                # Get logits for current sequence
                logits = self(generated)  # [1, seq_len, vocab_size]
                
                # Get logits for last position
                next_token_logits = logits[:, -1, :] / temperature
                
                # Sample
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # Append to sequence
                generated = torch.cat([generated, next_token], dim=1)
                
                # Stop if sequence too long
                if generated.size(1) >= self.max_seq_len:
                    break
        
        return generated
