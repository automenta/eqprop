"""
FeedbackAlignmentEqProp - Asymmetric Weights (Bio-Plausible)

Solves the "Weight Transport Problem":
- Forward weights W and backward weights B are DIFFERENT
- B is random and fixed (or slowly evolving)
- Network still learns because W adapts to align with B

Reference: Lillicrap et al., 2016 - "Random synaptic feedback weights 
support error backpropagation for deep learning"
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict
from torch.nn.utils.parametrizations import spectral_norm


class FeedbackAlignmentLayer(nn.Module):
    """Linear layer with separate forward and feedback weights."""
    
    def __init__(
        self,
        in_features: int,
        out_features: int,
        feedback_mode: str = 'random',  # 'random', 'evolving', 'symmetric'
    ):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.feedback_mode = feedback_mode
        
        # Forward weight (trained)
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # Feedback weight (fixed random or slowly evolved)
        if feedback_mode == 'random':
            # Fixed random feedback
            self.register_buffer('feedback_weight', torch.randn(in_features, out_features))
        elif feedback_mode == 'evolving':
            # Slowly trained feedback
            self.feedback_weight = nn.Parameter(torch.randn(in_features, out_features))
        else:  # symmetric (standard backprop)
            self.feedback_weight = None
        
        self._init_weights()
    
    def _init_weights(self):
        nn.init.xavier_uniform_(self.weight, gain=0.8)
        if hasattr(self, 'feedback_weight') and self.feedback_weight is not None:
            nn.init.xavier_uniform_(self.feedback_weight, gain=0.5)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight, self.bias)
    
    def get_feedback_weight(self) -> torch.Tensor:
        """Get the feedback weight matrix."""
        if self.feedback_mode == 'symmetric' or self.feedback_weight is None:
            return self.weight.t()
        return self.feedback_weight
    
    def get_alignment_angle(self) -> float:
        """Compute angle between forward and feedback weights."""
        W_flat = self.weight.t().flatten()
        B_flat = self.get_feedback_weight().flatten()
        cos_sim = F.cosine_similarity(W_flat.unsqueeze(0), B_flat.unsqueeze(0))
        return cos_sim.item()


class FeedbackAlignmentEqProp(nn.Module):
    """
    Equilibrium Propagation with Feedback Alignment.
    
    Uses asymmetric weights: forward W and feedback B.
    Proves EqProp can work without the biologically implausible
    requirement of symmetric weights (weight transport).
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int = 3,
        alpha: float = 0.5,
        feedback_mode: str = 'random',
        use_spectral_norm: bool = True,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.alpha = alpha
        self.feedback_mode = feedback_mode
        
        # Input projection
        self.W_in = nn.Linear(input_dim, hidden_dim)
        if use_spectral_norm:
            self.W_in = spectral_norm(self.W_in)
        
        # Hidden layers with feedback alignment
        self.layers = nn.ModuleList([
            FeedbackAlignmentLayer(hidden_dim, hidden_dim, feedback_mode)
            for _ in range(num_layers)
        ])
        
        # Output
        self.head = nn.Linear(hidden_dim, output_dim)
    
    def forward_step(self, h: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        """Single equilibrium step."""
        x_proj = self.W_in(x)
        
        for layer in self.layers:
            h = (1 - self.alpha) * h + self.alpha * torch.tanh(x_proj + layer(h))
        
        return h
    
    def forward(self, x: torch.Tensor, steps: int = 30) -> torch.Tensor:
        """Forward pass to equilibrium."""
        batch_size = x.size(0)
        h = torch.zeros(batch_size, self.hidden_dim, device=x.device)
        
        for _ in range(steps):
            h = self.forward_step(h, x)
        
        return self.head(h)
    
    def get_alignment_angles(self) -> Dict[str, float]:
        """Get alignment angles for all layers."""
        angles = {}
        for i, layer in enumerate(self.layers):
            angles[f'layer_{i}'] = layer.get_alignment_angle()
        return angles
    
    def get_mean_alignment(self) -> float:
        """Get mean alignment across all layers."""
        angles = self.get_alignment_angles()
        return sum(angles.values()) / len(angles)
