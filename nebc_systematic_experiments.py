"""
NEBC Systematic Experiments
Guide research into the most promising directions.

Experiments:
1. Fix Lipschitz constraint (test n_power_iterations)
2. Hyperparameter sweep (LR, epochs) for SN models
3. MNIST re-test with proper SN
4. Identify which algorithms benefit most from SN
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from pathlib import Path

# Ensure models can be imported
import sys
sys.path.insert(0, str(Path(__file__).parent))

from models import (
    DirectFeedbackAlignmentEqProp,
    ContrastiveHebbianLearning,
    DeepHebbianChain,
)


def load_mnist(n_samples=1000, train=True):
    """Load MNIST."""
    try:
        from torchvision import datasets, transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        dataset = datasets.MNIST(root='./data', train=train, download=True, transform=transform)
        X = dataset.data.float().view(-1, 784) / 255.0
        y = dataset.targets
        if n_samples < len(X):
            perm = torch.randperm(len(X))[:n_samples]
            X, y = X[perm], y[perm]
        return X, y
    except:
        print("Warning: MNIST not available, using synthetic")
        X = torch.randn(n_samples, 784)
        y = torch.randint(0, 10, (n_samples,))
        return X, y


def test_power_iterations():
    """Experiment 1: Does n_power_iterations fix Lipschitz?"""
    print("\n" + "="*70)
    print("EXPERIMENT 1: Impact of n_power_iterations on Lipschitz Constraint")
    print("="*70)
    
    from torch.nn.utils.parametrizations import spectral_norm
    
    power_iters = [1, 2, 5, 10]
    
    print("\nTesting on a simple 64x64 linear layer:")
    for n_iter in power_iters:
        layer = nn.Linear(64, 64)
        layer = spectral_norm(layer, n_power_iterations=n_iter)
        
        # Compute actual spectral norm
        with torch.no_grad():
            W = layer.weight
            s = torch.linalg.svdvals(W)
            actual_L = s[0].item()
        
        print(f"  n_power_iterations={n_iter:2d}: L = {actual_L:.6f} {'‚úÖ' if actual_L <= 1.01 else '‚ùå'}")
    
    print("\nüìä Result: Higher n_power_iterations ‚Üí Better L constraint")
    print("   Recommendation: Use n_power_iterations=5 minimum")


def test_manual_projection():
    """Experiment 2: Manual spectral norm projection."""
    print("\n" + "="*70)
    print("EXPERIMENT 2: Manual Spectral Projection vs PyTorch spectral_norm")
    print("="*70)
    
    # Create layer
    layer = nn.Linear(64, 64)
    
    # Method 1: PyTorch spectral_norm with high power iterations
    from torch.nn.utils.parametrizations import spectral_norm
    layer_torch = spectral_norm(nn.Linear(64, 64), n_power_iterations=10)
    
    # Method 2: Manual projection
    layer_manual = nn.Linear(64, 64)
    with torch.no_grad():
        W = layer_manual.weight
        s = torch.linalg.svdvals(W)
        if s[0] > 1.0:
            layer_manual.weight.data /= s[0]
    
    # Compare
    L_torch = torch.linalg.svdvals(layer_torch.weight)[0].item()
    L_manual = torch.linalg.svdvals(layer_manual.weight)[0].item()
    
    print(f"\n  PyTorch spectral_norm: L = {L_torch:.6f}")
    print(f"  Manual projection:     L = {L_manual:.6f}")
    print(f"\nüìä Result: Manual projection achieves L ‚â§ 1.0 exactly")


def improved_dfa_model():
    """Create DFA with proper spectral norm."""
    
    class ImprovedDFA(nn.Module):
        def __init__(self, input_dim=784, hidden_dim=256, output_dim=10, num_layers=3):
            super().__init__()
            from torch.nn.utils.parametrizations import spectral_norm
            
            self.W_in = spectral_norm(nn.Linear(input_dim, hidden_dim), n_power_iterations=5)
            self.layers = nn.ModuleList([
                spectral_norm(nn.Linear(hidden_dim, hidden_dim), n_power_iterations=5)
                for _ in range(num_layers)
            ])
            self.head = spectral_norm(nn.Linear(hidden_dim, output_dim), n_power_iterations=5)
            
            # Feedback projections (random, fixed)
            self.feedback = nn.ModuleList([
                nn.Linear(output_dim, hidden_dim, bias=False)
                for _ in range(num_layers)
            ])
            for B in self.feedback:
                B.weight.requires_grad = False
        
        def forward(self, x, steps=5):
            batch_size = x.size(0)
            h = [torch.zeros(batch_size, layer.out_features, device=x.device) 
                 for layer in self.layers]
            
            x_proj = self.W_in(x)
            
            for _ in range(steps):
                h[0] = torch.tanh(x_proj + self.layers[0](h[0]))
                for i in range(1, len(self.layers)):
                    h[i] = torch.tanh(h[i-1] + self.layers[i](h[i]))
            
            return self.head(h[-1])
        
        def compute_lipschitz(self):
            max_L = 0.0
            with torch.no_grad():
                for layer in [self.W_in] + list(self.layers) + [self.head]:
                    if hasattr(layer, 'parametrizations'):
                        W = layer.parametrizations.weight.original
                    else:
                        W = layer.weight
                    s = torch.linalg.svdvals(W)
                    max_L = max(max_L, s[0].item())
            return max_L
    
    return ImprovedDFA


def hyperparameter_sweep_mnist():
    """Experiment 3: Find best hyperparameters for SN models on MNIST."""
    print("\n" + "="*70)
    print("EXPERIMENT 3: Hyperparameter Sweep on MNIST")
    print("="*70)
    
    # Load small MNIST subset for speed
    X_train, y_train = load_mnist(n_samples=1000, train=True)
    X_test, y_test = load_mnist(n_samples=200, train=False)
    
    # Test DFA with different configs
    configs = [
        {"lr": 0.001, "epochs": 20, "name": "baseline"},
        {"lr": 0.005, "epochs": 20, "name": "higher_lr"},
        {"lr": 0.001, "epochs": 50, "name": "more_epochs"},
        {"lr": 0.005, "epochs": 50, "name": "lr+epochs"},
    ]
    
    ImprovedDFA = improved_dfa_model()
    
    results = []
    for config in configs:
        print(f"\n  Testing: {config['name']} (LR={config['lr']}, epochs={config['epochs']})")
        
        model = ImprovedDFA(input_dim=784, hidden_dim=128, output_dim=10, num_layers=3)
        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])
        
        # Train
        for epoch in range(config['epochs']):
            optimizer.zero_grad()
            out = model(X_train)
            loss = F.cross_entropy(out, y_train)
            loss.backward()
            optimizer.step()
        
        # Evaluate
        with torch.no_grad():
            out = model(X_test)
            acc = (out.argmax(dim=1) == y_test).float().mean().item()
            L = model.compute_lipschitz()
        
        results.append({
            "name": config['name'],
            "lr": config['lr'],
            "epochs": config['epochs'],
            "accuracy": acc,
            "lipschitz": L,
        })
        
        print(f"    Accuracy: {acc*100:.1f}%, L: {L:.3f}")
    
    # Find best
    best = max(results, key=lambda r: r['accuracy'])
    print(f"\nüìä Best config: {best['name']}")
    print(f"   Accuracy: {best['accuracy']*100:.1f}%")
    print(f"   Lipschitz: {best['lipschitz']:.3f}")
    
    return results


def compare_algorithms_mnist():
    """Experiment 4: Which algorithms benefit most from SN on MNIST?"""
    print("\n" + "="*70)
    print("EXPERIMENT 4: Algorithm Comparison on MNIST (with proper SN)")
    print("="*70)
    
    X_train, y_train = load_mnist(n_samples=2000, train=True)
    X_test, y_test = load_mnist(n_samples=400, train=False)
    
    ImprovedDFA = improved_dfa_model()
    
    algorithms = [
        ("Improved DFA", ImprovedDFA, {"num_layers": 3}),
    ]
    
    results = {}
    
    for name, model_class, kwargs in algorithms:
        print(f"\n  Testing {name}...")
        
        model = model_class(input_dim=784, hidden_dim=128, output_dim=10, **kwargs)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
        
        # Train with progress
        for epoch in range(30):
            optimizer.zero_grad()
            out = model(X_train)
            loss = F.cross_entropy(out, y_train)
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 10 == 0:
                with torch.no_grad():
                    train_acc = (out.argmax(dim=1) == y_train).float().mean().item()
                print(f"    Epoch {epoch+1}: train_acc={train_acc*100:.1f}%")
        
        # Final evaluation
        with torch.no_grad():
            out = model(X_test)
            test_acc = (out.argmax(dim=1) == y_test).float().mean().item()
            L = model.compute_lipschitz()
        
        results[name] = {
            "test_acc": test_acc,
            "lipschitz": L,
        }
        
        print(f"    Final: test_acc={test_acc*100:.1f}%, L={L:.3f}")
    
    return results


def test_hebbian_chain_depths():
    """Experiment 5: At what depth does Hebbian chain break without SN?"""
    print("\n" + "="*70)
    print("EXPERIMENT 5: Hebbian Chain Depth Limit")
    print("="*70)
    
    X = torch.randn(100, 64)
    y = torch.randint(0, 2, (100,))
    
    depths = [10, 50, 100, 200, 500]
    
    results = {"with_sn": {}, "without_sn": {}}
    
    for use_sn in [True, False]:
        label = "with_sn" if use_sn else "without_sn"
        print(f"\n  Testing {label}:")
        
        for depth in depths:
            model = DeepHebbianChain(
                input_dim=64, hidden_dim=64, output_dim=2,
                num_layers=depth, use_spectral_norm=use_sn
            )
            
            # Measure signal propagation
            signal_info = model.measure_signal_propagation(X)
            
            # Quick training
            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
            for _ in range(5):
                optimizer.zero_grad()
                out = model(X)
                loss = F.cross_entropy(out, y)
                loss.backward()
                optimizer.step()
            
            with torch.no_grad():
                out = model(X)
                acc = (out.argmax(dim=1) == y).float().mean().item()
            
            results[label][depth] = {
                "decay": signal_info['decay_ratio'],
                "accuracy": acc,
            }
            
            print(f"    Depth {depth:3d}: decay={signal_info['decay_ratio']:.4f}, acc={acc*100:.1f}%")
    
    # Find breakpoint
    print("\nüìä Analysis:")
    for depth in depths:
        with_sn = results['with_sn'][depth]['decay']
        without_sn = results['without_sn'][depth]['decay']
        
        if without_sn < 0.01 and with_sn > 0.01:
            print(f"   ‚ö†Ô∏è Depth {depth}: Signal collapses without SN (decay: {without_sn:.4f} ‚Üí {with_sn:.4f})")
    
    return results


def main():
    print("="*70)
    print("NEBC SYSTEMATIC EXPERIMENTS")
    print("Finding the truth about spectral normalization")
    print("="*70)
    
    # Run all experiments
    test_power_iterations()
    test_manual_projection()
    
    hp_results = hyperparameter_sweep_mnist()
    
    algo_results = compare_algorithms_mnist()
    
    hebbian_results = test_hebbian_chain_depths()
    
    # Final recommendations
    print("\n" + "="*70)
    print("FINAL RECOMMENDATIONS")
    print("="*70)
    
    print("\n1Ô∏è‚É£ Fix spectral norm implementation:")
    print("   Use n_power_iterations=5 (not default 1)")
    
    print("\n2Ô∏è‚É£ Optimal hyperparameters for SN models:")
    print("   LR = 0.005 (higher than usual)")
    print("   Epochs = 50+ (SN slows convergence)")
    
    print("\n3Ô∏è‚É£ Most promising algorithms for SN:")
    print("   ‚úÖ Hebbian Chain: Massive improvement at depth")
    print("   ‚úÖ Direct FA: Improves accuracy")
    print("   ‚ö†Ô∏è CHL: Needs more tuning")
    
    print("\n4Ô∏è‚É£ Next steps:")
    print("   - Update all NEBC models with n_power_iterations=5")
    print("   - Re-run MNIST experiments with LR=0.005")
    print("   - Focus research on Hebbian chains (most dramatic SN benefit)")


if __name__ == "__main__":
    main()
