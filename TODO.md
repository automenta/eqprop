# Equilibrium Propagation Research Roadmap: Breakthrough Performance on Real-World Tasks

> **Mission**: Demonstrate that Spectral Normalization transforms EqProp into a production-ready training paradigm with unique advantages over Backpropagation.
>
> **Timeline**: 10 weeks | **Hardware**: Single commodity GPU (8GB VRAM) | **Feasibility**: 85-95% (see FEASIBILITY.md)

---

## Quick Start

```bash
# Verify existing 30 tracks work
python verify.py --quick

# List all tracks
python verify.py --list

# Run specific existing track
python verify.py --track 33  # CIFAR-10 baseline
```

---

## Theoretical Foundation

### The Core Mathematical Guarantee

**Banach Fixed-Point Theorem**: If $f: X \to X$ is a contraction with Lipschitz constant $L < 1$:
1. $f$ has a **unique** fixed point $x^*$
2. For any $x_0$, iteration $x_{n+1} = f(x_n)$ **converges** to $x^*$
3. Convergence rate: $||x_n - x^*|| \leq L^n ||x_0 - x^*||$

**Spectral Normalization** enforces $L \leq 1$ by constraining singular values:
```python
WÃÉ = W / œÉ_max(W)  # Maximum singular value ‚â§ 1
```

**Implication**: With SN, equilibrium is guaranteed in $O(\log(1/\epsilon))$ steps.

---

## Research Tracks Overview

| Track | Goal | Existing Base | New Validation | Priority |
|-------|------|---------------|----------------|----------|
| **A** | CIFAR-10 75%+ | Track 33: 44.5% | Track 34 | HIGH |
| **B** | O(1) Memory Demo | Track 26: Theory | Track 35 | HIGH |
| **C** | LM Perplexity | Track 14: Toy tasks | Tracks 37-38 | HIGH |
| **D** | Diffusion | Track 32: Gen working | Track 39 | MEDIUM |
| **E** | Energy Confidence | Track 29: Energy | Track 36 | HIGH |
| **F** | Hardware Analysis | Tracks 4,16-18 | Track 40 | MEDIUM |

---

## Track A: Core Stability & Scaling (Weeks 1-3)

### EXISTING FOUNDATION
- ‚úÖ Track 1: Spectral Norm maintains L<1 (100% pass)
- ‚úÖ Track 2: EqProp matches Backprop (100% pass)
- ‚úÖ Track 33: CIFAR-10 44.5% with LoopedMLP (92% pass)
- ‚úÖ Track 13: ConvEqProp works on synthetic shapes (100% pass)

### A1: Architecture-Agnostic Stability Study

**Hypothesis**: Spectral Normalization is necessary and sufficient across all architectures.

**Experiments** (5 seeds each):

```python
# models_to_test.py
from models import LoopedMLP, ConvEqProp, TransformerEqProp

experiments = [
    {"arch": "LoopedMLP", "dataset": "CIFAR10", "sn": True},
    {"arch": "LoopedMLP", "dataset": "CIFAR10", "sn": False},  # Expect divergence
    {"arch": "ConvEqProp", "dataset": "CIFAR10", "sn": True},
    {"arch": "ConvEqProp", "dataset": "CIFAR10", "sn": False},
    # ... repeat for Transformer, DeepEqProp
]

# Track Lipschitz constant L(t) throughout training
# Statistical test: paired t-test, p < 0.05
```

**Deliverable**: L(t) trajectory plots showing:
- SN models: L stays below 1.0
- No-SN models: L diverges to 5-20
- Phase diagram: L threshold vs training stability

**Success Criteria**: 
- All SN models: L < 1.1 throughout training
- All no-SN models: L > 2.0 or training diverges
- Statistical significance: p < 0.01

**Time Estimate**: 5-7 days

---

### A2: CIFAR-10 Scaling to 75%+ ‚≠ê NEW VALIDATION TRACK 34

**Current State**: 44.5% with LoopedMLP (fully-connected)

**Problem**: Need proper convolutional architecture

**Solution**: Multi-stage ConvEqProp with spectral norm

**Implementation**:

```python
# models/modern_conv_eqprop.py
import torch.nn as nn
from torch.nn.utils.parametrizations import spectral_norm
from .utils import spectral_conv2d

class ModernConvEqProp(nn.Module):
    """
    ResNet-inspired ConvEqProp with equilibrium settling.
    
    Architecture:
        Input: 3x32x32 (CIFAR-10)
        Stage 1: Conv 3‚Üí64, no pooling (32x32)
        Stage 2: Conv 64‚Üí128, stride 2 (16x16)
        Stage 3: Conv 128‚Üí256, stride 2 (8x8)
        Equilibrium: Recurrent conv at 256 channels
        Output: Global pool ‚Üí Linear(256, 10)
    """
    def __init__(self, eq_steps=15, gamma=0.5):
        super().__init__()
        self.eq_steps = eq_steps
        self.gamma = gamma
        
        # Feature extraction stages (non-recurrent)
        self.stage1 = nn.Sequential(
            spectral_conv2d(3, 64, 3, padding=1, use_sn=True),
            nn.GroupNorm(8, 64),
            nn.Tanh()
        )
        
        self.stage2 = nn.Sequential(
            spectral_conv2d(64, 128, 3, stride=2, padding=1, use_sn=True),
            nn.GroupNorm(8, 128),
            nn.Tanh()
        )
        
        self.stage3 = nn.Sequential(
            spectral_conv2d(128, 256, 3, stride=2, padding=1, use_sn=True),
            nn.GroupNorm(8, 256),
            nn.Tanh()
        )
        
        # Equilibrium recurrent block
        self.eq_conv = spectral_conv2d(256, 256, 3, padding=1, use_sn=True)
        self.eq_norm = nn.GroupNorm(8, 256)
        
        # Output
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, 10)
        
    def forward(self, x, steps=None):
        steps = steps or self.eq_steps
        
        # Non-recurrent feature extraction
        h = self.stage1(x)
        h = self.stage2(h)
        h = self.stage3(h)
        
        # Equilibrium settling
        for _ in range(steps):
            h_norm = self.eq_norm(h)
            h_next = torch.tanh(self.eq_conv(h_norm))
            h = (1 - self.gamma) * h + self.gamma * h_next
        
        # Classification
        features = self.pool(h).flatten(1)
        return self.fc(features)
```

**Training Protocol**:

```python
# experiments/cifar_breakthrough.py
import torch
from torchvision import datasets, transforms
from models import ModernConvEqProp

# Data augmentation
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                         (0.2470, 0.2435, 0.2616))
])

# Hyperparameter search space
configs = [
    {"channels": 64, "steps": 10, "lr": 0.001},
    {"channels": 128, "steps": 15, "lr": 0.001},  # Recommended
    {"channels": 256, "steps": 20, "lr": 0.0005},
]

# Train with 3 seeds for statistical significance
for seed in [42, 123, 456]:
    for config in configs:
        model = ModernConvEqProp(**config)
        # Train for 100 epochs
        # Report: mean ¬± std accuracy
```

**Baseline Comparison**:

| Model | Params | CIFAR-10 Acc | Notes |
|-------|--------|--------------|-------|
| LoopedMLP (current) | 2M | 44.5% | Fully-connected |
| ResNet-18 | 11M | ~93% | Standard baseline |
| **Target: ModernConvEqProp** | 5M | **75%+** | Must exceed MLP |

**Success Criteria**:
- Mean accuracy ‚â• 75% (3 seeds)
- Training time < 4 hours on 8GB GPU
- Memory usage < 4GB
- Statistical significance vs LoopedMLP baseline

**Time Estimate**: 8-10 days

---

## Track B: O(1) Memory Scaling (Week 4)

### EXISTING FOUNDATION
- ‚úÖ Track 26: O(1) memory proven theoretically
- ‚úÖ Track 23: 500 layers trained successfully

### B: Memory Scaling Demonstration ‚≠ê NEW VALIDATION TRACK 35

**Hypothesis**: EqProp achieves O(1) memory w.r.t. depth using gradient checkpointing.

**Challenge**: PyTorch autograd stores activations ‚Üí O(T) memory

**Solution**: Gradient checkpointing (‚àöD memory, practical compromise)

**Implementation**:

```python
# experiments/memory_scaling_demo.py
import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint
import psutil
import os

class DeepEqPropCheckpointed(nn.Module):
    """Deep network with gradient checkpointing."""
    def __init__(self, depth, hidden=256):
        super().__init__()
        self.input = nn.Linear(3072, hidden)
        self.layers = nn.ModuleList([
            nn.Linear(hidden, hidden) for _ in range(depth)
        ])
        self.output = nn.Linear(hidden, 10)
        
        # Apply spectral norm to all layers
        for layer in [self.input] + list(self.layers) + [self.output]:
            layer = spectral_norm(layer)
    
    def forward(self, x):
        h = torch.tanh(self.input(x.view(x.size(0), -1)))
        
        # Checkpoint every sqrt(depth) layers
        checkpoint_freq = max(1, int(len(self.layers) ** 0.5))
        
        for i, layer in enumerate(self.layers):
            if i % checkpoint_freq == 0:
                h = checkpoint(lambda h: torch.tanh(layer(h)), h)
            else:
                h = torch.tanh(layer(h))
        
        return self.output(h)

def measure_memory(model, batch_size=128):
    """Measure peak GPU memory during backward pass."""
    x = torch.randn(batch_size, 3, 32, 32).cuda()
    y = torch.randint(0, 10, (batch_size,)).cuda()
    
    torch.cuda.reset_peak_memory_stats()
    
    out = model(x)
    loss = nn.functional.cross_entropy(out, y)
    loss.backward()
    
    peak_memory = torch.cuda.max_memory_allocated() / 1e6  # MB
    return peak_memory

# Experiment: Vary depth, measure memory
results = []
for depth in [10, 25, 50, 100, 200, 500]:
    # EqProp with checkpointing
    model_eqprop = DeepEqPropCheckpointed(depth).cuda()
    mem_eqprop = measure_memory(model_eqprop)
    
    # Standard Backprop (compare if fits in memory)
    try:
        model_backprop = StandardDeepMLP(depth).cuda()
        mem_backprop = measure_memory(model_backprop)
        oom = False
    except RuntimeError:  # OOM
        mem_backprop = None
        oom = True
    
    results.append({
        "depth": depth,
        "eqprop_memory_mb": mem_eqprop,
        "backprop_memory_mb": mem_backprop,
        "backprop_oom": oom
    })

# Plot: Memory vs Depth
# EqProp: Sublinear (‚àöD)
# Backprop: Linear (D), OOM at ~50-100 layers
```

**Expected Results**:

| Depth | EqProp Memory | Backprop Memory | Backprop Status |
|-------|---------------|-----------------|-----------------|
| 10 | 150 MB | 200 MB | ‚úÖ |
| 25 | 200 MB | 500 MB | ‚úÖ |
| 50 | 250 MB | 1000 MB | ‚úÖ |
| 100 | 350 MB | 2000 MB | ‚úÖ |
| 200 | 500 MB | 4000 MB | ‚ö†Ô∏è Near OOM |
| 500 | 800 MB | - | ‚ùå OOM |

**Deliverable**: 
- Memory vs depth plot (log scale)
- Table showing OOM threshold
- Documentation of checkpointing strategy

**Success Criteria**:
- Train 200+ layers without OOM on 8GB GPU
- Memory growth: O(‚àöD) or better
- Backprop OOMs at <100 layers

**Time Estimate**: 3-5 days

**Note**: Custom CUDA kernel for true O(1) is future work (4+ weeks)

---

## Track C: Language Modeling (Weeks 5-7)

### EXISTING FOUNDATION
- ‚úÖ Track 14: TransformerEqProp 99.9% on sequence reversal

### C1: Character-Level Language Modeling ‚≠ê NEW VALIDATION TRACK 37

**Hypothesis**: TransformerEqProp matches standard Transformer perplexity.

**Datasets**:
- Shakespeare (1MB, 65 chars)
- WikiText-2 (200MB, vocab ~33k)
- Penn Treebank (5MB, vocab ~10k)

**Implementation**:

```python
# models/causal_transformer_eqprop.py
from models.transformer import TransformerEqProp

class CausalTransformerEqProp(TransformerEqProp):
    """TransformerEqProp with causal masking for LM."""
    
    def __init__(self, vocab_size, hidden_dim=256, num_layers=4, 
                 num_heads=4, max_seq_len=512, eq_steps=20):
        super().__init__(vocab_size, hidden_dim, vocab_size,  # output_dim = vocab_size
                         num_layers, num_heads, max_seq_len, alpha=0.5)
        self.eq_steps = eq_steps
        
        # Replace classification head with LM head
        self.lm_head = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x, steps=None):
        steps = steps or self.eq_steps
        batch_size, seq_len = x.shape
        
        # Token + position embeddings
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        h = self.token_emb(x) + self.pos_emb(positions)
        
        # Equilibrium settling with causal mask
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        
        for _ in range(steps):
            for i, (attn, ffn, norm1, norm2) in enumerate(
                zip(self.attentions, self.ffns, self.norms1, self.norms2)
            ):
                # Self-attention with causal masking
                h_norm = norm1(h)
                h = h + attn(h_norm, causal_mask=causal_mask)
                
                # FFN
                h_norm = norm2(h)
                h = h + ffn(h_norm)
        
        # LM head
        return self.lm_head(h)  # [batch, seq, vocab]
```

**Training Protocol**:

```python
# experiments/language_modeling.py
import torch
from torch.utils.data import DataLoader

# Dataset: Shakespeare
with open('shakespeare.txt', 'r') as f:
    text = f.read()
chars = sorted(set(text))
char_to_idx = {ch: i for i, ch in enumerate(chars)}
vocab_size = len(chars)

# Tokenize
data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)

# Split: 90% train, 10% val
n = int(0.9 * len(data))
train_data, val_data = data[:n], data[n:]

# Create sequences
def get_batch(split, seq_len=128, batch_size=64):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - seq_len, (batch_size,))
    x = torch.stack([data[i:i+seq_len] for i in ix])
    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])
    return x, y

# Model
model = CausalTransformerEqProp(vocab_size, hidden_dim=256, num_layers=4)

# Train
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
for epoch in range(50):
    x, y = get_batch('train')
    logits = model(x)  # [batch, seq, vocab]
    loss = nn.functional.cross_entropy(
        logits.view(-1, vocab_size), 
        y.view(-1)
    )
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Validation
    if epoch % 10 == 0:
        with torch.no_grad():
            x_val, y_val = get_batch('val')
            logits_val = model(x_val)
            val_loss = nn.functional.cross_entropy(
                logits_val.view(-1, vocab_size),
                y_val.view(-1)
            )
            perplexity = torch.exp(val_loss).item()
            print(f"Epoch {epoch}, Val Perplexity: {perplexity:.2f}")
```

**Baseline Comparison**:

| Model | Params | Shakespeare Perplexity | WikiText-2 Perplexity |
|-------|--------|------------------------|----------------------|
| Transformer-4L | 10M | 1.5-2.0 | 40-50 |
| **TransformerEqProp-4L** | 10M | **<2.5** | **<60** |

**Success Criteria**:
- Shakespeare perplexity < 2.5 (within 25% of baseline)
- WikiText-2 perplexity < 60 (within 20% of baseline)
- 3 seeds, report mean ¬± std

**Time Estimate**: 8-10 days

---

### C2: Adaptive Compute Analysis ‚≠ê NEW VALIDATION TRACK 38

**Hypothesis**: Equilibrium settling time correlates with sequence complexity.

**Unique Advantage of EqProp**: Standard Transformers use fixed compute per token. EqProp can "think longer" on hard inputs.

**Experiment**:

```python
# experiments/adaptive_compute.py
import torch
import numpy as np
from scipy.stats import pearsonr

def measure_settling_time(model, x, max_steps=100, epsilon=1e-5):
    """Measure steps until convergence."""
    model.eval()
    h_prev = None
    
    for step in range(max_steps):
        with torch.no_grad():
            h = model.forward_one_step(x)  # Single equilibrium step
            
            if h_prev is not None:
                delta = (h - h_prev).norm().item()
                if delta < epsilon:
                    return step  # Converged
            h_prev = h
    
    return max_steps  # Did not converge

# Analyze correlation
model = CausalTransformerEqProp(vocab_size, hidden_dim=256)
model.load_state_dict(torch.load('trained_model.pth'))

results = []
for i in range(1000):  # Sample 1000 sequences
    x, y = get_batch('val', seq_len=128, batch_size=1)
    
    # Measure settling time
    steps_to_converge = measure_settling_time(model, x)
    
    # Measure sequence complexity (ground truth perplexity)
    with torch.no_grad():
        logits = model(x)
        loss = nn.functional.cross_entropy(
            logits.view(-1, vocab_size), y.view(-1)
        )
        complexity = torch.exp(loss).item()
    
    results.append({
        "settling_time": steps_to_converge,
        "complexity": complexity
    })

# Correlation analysis
settling_times = [r["settling_time"] for r in results]
complexities = [r["complexity"] for r in results]

r, p = pearsonr(settling_times, complexities)
print(f"Correlation: r={r:.3f}, p={p:.4f}")

# Visualize
import matplotlib.pyplot as plt
plt.scatter(complexities, settling_times, alpha=0.5)
plt.xlabel("Sequence Complexity (Perplexity)")
plt.ylabel("Settling Time (Steps)")
plt.title(f"Adaptive Compute: r={r:.3f}")
plt.savefig("adaptive_compute.png")
```

**Expected Results**:
- Pearson r > 0.5 (moderate positive correlation)
- p < 0.001 (statistically significant)
- Hard sequences (high perplexity) ‚Üí longer settling

**Deliverable**:
- Scatter plot: complexity vs settling time
- Statistical analysis (correlation coefficient)
- Examples of fast-settling (simple) vs slow-settling (complex) sequences

**Success Criteria**:
- r > 0.4 (at least weak correlation)
- p < 0.01 (significant)

**Time Estimate**: 3-5 days

---

## Track D: Energy-Based Diffusion (Weeks 7-8) [OPTIONAL]

### EXISTING FOUNDATION
- ‚úÖ Track 32: Bidirectional generation (100% pass)
- ‚úÖ Track 29: Energy dynamics converge

### D: EqProp Diffusion ‚≠ê NEW VALIDATION TRACK 39 [STRETCH GOAL]

**Hypothesis**: Denoising diffusion is energy minimization ‚Üí natural EqProp application.

**Theoretical Unification**:

```
Standard Diffusion: Learn Œµ_Œ∏(x_t, t) to predict noise
Energy Formulation: E(x,t) = ||x - Denoise(x_t,t)||¬≤ + ŒªR(x)

Equilibrium Denoising: x_{k+1} = x_k - Œ∑‚àá_x E(x_k, t)
```

**Implementation** (simplified):

```python
# models/eqprop_diffusion.py
import torch
import torch.nn as nn
from models import ConvEqProp

class EqPropDiffusion(nn.Module):
    def __init__(self, img_channels=1, hidden_channels=64):
        super().__init__()
        self.denoiser = ConvEqProp(
            input_channels=img_channels + 1,  # +1 for time embedding
            hidden_channels=hidden_channels,
            output_dim=img_channels * 28 * 28
        )
        
    def energy(self, x_noisy, x_pred, t):
        """Energy function for denoising."""
        recon_error = ((x_noisy - x_pred) ** 2).sum()
        # Simple prior: penalize high-frequency noise
        prior = self.total_variation(x_pred)
        return recon_error + 0.1 * prior
    
    def total_variation(self, x):
        """Smoothness prior."""
        dx = (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().sum()
        dy = (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().sum()
        return dx + dy
    
    def denoise_step(self, x_noisy, t, steps=10):
        """Single denoising step via equilibrium."""
        # Embed time
        t_emb = torch.ones(x_noisy.size(0), 1, 28, 28) * t
        x_input = torch.cat([x_noisy, t_emb], dim=1)
        
        # Iterate to equilibrium
        h = x_noisy
        for _ in range(steps):
            h_pred = self.denoiser(x_input).view_as(x_noisy)
            # Gradient descent on energy
            grad = 2 * (h - h_pred)  # ‚àáE = 2(x - x_pred)
            h = h - 0.1 * grad
        
        return h
```

**Training** (DDPM-style):

```python
# experiments/diffusion_mnist.py
from torchvision import datasets, transforms

# MNIST
train_data = datasets.MNIST(root='./data', train=True, download=True,
                            transform=transforms.ToTensor())

model = EqPropDiffusion()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Noise schedule
T = 1000
beta = torch.linspace(1e-4, 0.02, T)
alpha = 1 - beta
alpha_bar = torch.cumprod(alpha, dim=0)

for epoch in range(100):
    for x, _ in train_loader:
        # Sample timestep
        t = torch.randint(0, T, (x.size(0),))
        
        # Add noise
        noise = torch.randn_like(x)
        x_noisy = torch.sqrt(alpha_bar[t]) * x + torch.sqrt(1 - alpha_bar[t]) * noise
        
        # Denoise via equilibrium
        x_pred = model.denoise_step(x_noisy, t / T)
        
        # Loss: predict original x
        loss = ((x_pred - x) ** 2).mean()
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**Success Criteria** (MNIST):
- Generate recognizable digits (subjective)
- FID < 50 (not SOTA, proof of concept)
- Demonstrate energy‚Üîquality correlation

**Risk**: HIGH - Diffusion is complex, may not converge
**Mitigation**: Start simple (MNIST), accept lower quality

**Time Estimate**: 10-15 days

**Recommendation**: Consider this optional/stretch goal. Focus on Tracks A-C first.

---

## Track E: Energy-Based Confidence (Week 9)

### EXISTING FOUNDATION
- ‚úÖ Track 29: Energy dynamics proven
- ‚úÖ Track 3: Self-healing via L<1

### E: OOD Detection via Energy ‚≠ê NEW VALIDATION TRACK 36

**Hypothesis**: Energy-based confidence outperforms Softmax for OOD detection.

**Implementation**:

```python
# experiments/energy_confidence.py
import torch
import numpy as np
from sklearn.metrics import roc_auc_score

def energy_score(model, x):
    """Compute energy-based confidence score."""
    # Run to equilibrium, track settling
    h_trajectory = []
    h = torch.zeros(x.size(0), model.hidden_dim)
    
    for step in range(30):
        h_new = model.equilibrium_step(x, h)
        h_trajectory.append(h_new)
        h = h_new
    
    # Final energy
    energy = model.compute_energy(h, x)
    
    # Settling time (steps to converge)
    settling_time = 30
    for i in range(1, len(h_trajectory)):
        if (h_trajectory[i] - h_trajectory[i-1]).norm() < 1e-5:
            settling_time = i
            break
    
    # Combined score: lower energy + faster settling = higher confidence
    score = -energy.mean().item() / (settling_time + 1)
    return score

# OOD Detection Experiment
model = LoopedMLP(...).load_state_dict(...)

# In-distribution: CIFAR-10
cifar10_test = datasets.CIFAR10(root='./data', train=False)

# Out-of-distribution: SVHN, CIFAR-100, Gaussian noise
svhn = datasets.SVHN(root='./data', split='test')
cifar100 = datasets.CIFAR100(root='./data', train=False)
gaussian_noise = torch.randn(10000, 3, 32, 32)

# Compute scores
id_scores = [energy_score(model, x) for x in cifar10_test]
ood_svhn_scores = [energy_score(model, x) for x in svhn]
ood_cifar100_scores = [energy_score(model, x) for x in cifar100]
ood_noise_scores = [energy_score(model, gaussian_noise[i:i+128]) 
                    for i in range(0, 10000, 128)]

# AUROC
labels_svhn = [0]*len(id_scores) + [1]*len(ood_svhn_scores)
scores_svhn = id_scores + ood_svhn_scores
auroc_svhn = roc_auc_score(labels_svhn, scores_svhn)

print(f"CIFAR-10 vs SVHN AUROC: {auroc_svhn:.3f}")

# Compare to Softmax baseline
softmax_scores = [torch.softmax(model(x), dim=-1).max() for x in ...]
softmax_auroc = roc_auc_score(labels, softmax_scores)

print(f"Energy AUROC: {auroc_svhn:.3f}")
print(f"Softmax AUROC: {softmax_auroc:.3f}")
```

**Expected Results**:

| OOD Dataset | Energy AUROC | Softmax AUROC | Improvement |
|-------------|--------------|---------------|-------------|
| SVHN | 0.85-0.90 | 0.70 | +15-20% |
| CIFAR-100 | 0.80-0.85 | 0.60 | +20-25% |
| Gaussian | 0.95+ | 0.90 | +5% |

**Success Criteria**:
- Energy AUROC ‚â• 0.85 on at least 2/3 OOD datasets
- Statistically significant improvement over Softmax (p < 0.05)

**Time Estimate**: 5-7 days

---

## Track F: Hardware & Efficiency (Week 10)

### EXISTING FOUNDATION
- ‚úÖ Track 4: Ternary weights (70% sparsity, 99% accuracy)
- ‚úÖ Track 16: INT8 quantization robust
- ‚úÖ Track 17: 5% analog noise tolerant
- ‚úÖ Track 18: Metabolic cost minimization

### F: Comprehensive Hardware Analysis ‚≠ê NEW VALIDATION TRACK 40

**This is mostly analysis of existing results + new FLOP measurement**

**F1: Quantization Robustness** (Already proven in Tracks 4, 16)

Just consolidate into table:

| Precision | Weights | Activations | Accuracy Drop | Hardware Benefit |
|-----------|---------|-------------|---------------|------------------|
| FP32 | FP32 | FP32 | 0% (baseline) | - |
| INT8 | INT8 | INT8 | <1% ‚úÖ | 4√ó memory, 2-4√ó speed |
| INT4 | INT4 | INT8 | <3% ‚úÖ | 8√ó memory |
| Ternary | {-1,0,+1} | FP32 | <1% ‚úÖ | 32√ó memory, no FPU |

**F2: Analog Noise Tolerance** (Track 17 proven)

Already shown: 5% noise ‚Üí minimal accuracy drop

**F3: FLOP Analysis** (NEW)

```python
# experiments/flop_analysis.py
from torch.profiler import profile, ProfilerActivity

def count_flops(model, x):
    """Count FLOPs using PyTorch profiler."""
    with profile(activities=[ProfilerActivity.CPU], 
                 record_shapes=True) as prof:
        model(x)
    
    # Sum FLOPs from profile
    total_flops = sum([event.flops for event in prof.key_averages()])
    return total_flops

# Compare EqProp vs Backprop
model_eqprop = LoopedMLP(..., max_steps=30)
model_backprop = BackpropMLP(...)

x = torch.randn(128, 784)

flops_eqprop = count_flops(model_eqprop, x)
flops_backprop = count_flops(model_backprop, x)

print(f"EqProp FLOPs: {flops_eqprop / 1e9:.2f}G")
print(f"Backprop FLOPs: {flops_backprop / 1e9:.2f}G")
print(f"Ratio: {flops_eqprop / flops_backprop:.1f}√ó")
```

**Expected**: EqProp is 30-50√ó more FLOPs due to equilibrium iterations

**Deliverable**: Comprehensive hardware readiness table

**Time Estimate**: 2-3 days

---

## Validation Track Summary

| New Track | Name | Based On | Effort | Priority |
|-----------|------|----------|--------|----------|
| 34 | ConvEqProp CIFAR-10 75%+ | Tracks 13, 33 | 10 days | HIGH |
| 35 | O(1) Memory Scaling | Tracks 23, 26 | 5 days | HIGH |
| 36 | Energy OOD Detection | Track 29 | 7 days | HIGH |
| 37 | Character LM Perplexity | Track 14 | 10 days | HIGH |
| 38 | Adaptive Compute | Track 14 | 5 days | MEDIUM |
| 39 | EqProp Diffusion | Track 32 | 15 days | LOW (optional) |
| 40 | Hardware Analysis | Tracks 4,16-18 | 3 days | MEDIUM |

**Total**: 5-7 new tracks (depending on stretch goals)

---

## Statistical Rigor Protocol

### All Experiments Must Follow

1. **Multiple Seeds**: Minimum 3 seeds {42, 123, 456}
2. **Confidence Intervals**: Report mean ¬± 1.96¬∑SE for 95% CI
3. **Statistical Tests**: 
   - Comparisons: Paired t-test
   - Multiple comparisons: Bonferroni correction
4. **Effect Size**: Report Cohen's d alongside p-values

### Example Reporting

```python
# Good: Statistical rigor
results = run_experiment(seeds=[42, 123, 456, 789, 1000])
mean_acc = np.mean(results)
std_acc = np.std(results)
se_acc = std_acc / np.sqrt(len(results))
ci_95 = 1.96 * se_acc

print(f"Accuracy: {mean_acc*100:.1f}% ¬± {ci_95*100:.1f}% (95% CI)")
print(f"n={len(results)} seeds")
```

---

## Timeline & Milestones

| Week | Focus | Deliverables | Risk |
|------|-------|--------------|------|
| 1-2 | Track A1-A2 | L(t) plots, CIFAR-10 ‚â•70% | LOW |
| 3 | Track A2 cont | CIFAR-10 ‚â•75%, Track 34 | MEDIUM |
| 4 | Track B | Memory scaling, Track 35 | LOW |
| 5-6 | Track C1 | LM perplexity, Track 37 | LOW |
| 7 | Track C2 | Adaptive compute, Track 38 | LOW |
| 8 | Track E | OOD detection, Track 36 | LOW |
| 9-10 | Track F + D | Hardware analysis (Track 40), optional diffusion | MEDIUM |

**Critical Path**: Tracks A2 ‚Üí B ‚Üí C1 (must succeed for publication)

---

## Success Criteria

### Minimum Viable (90% probability) ‚úÖ

- [ ] CIFAR-10 ‚â• 70% with ConvEqProp
- [ ] Memory scaling plot (‚àöD growth)
- [ ] LM perplexity within 30% of baseline
- [ ] OOD AUROC ‚â• 0.80
- [ ] 5 new validation tracks (34-38)

### Target (70% probability) üéØ

- [ ] CIFAR-10 ‚â• 75%
- [ ] Train 200+ layers on 8GB GPU
- [ ] LM perplexity within 20%
- [ ] OOD AUROC ‚â• 0.85
- [ ] 6 new validation tracks (34-40, no diffusion)

### Stretch (40% probability) ‚≠ê

- [ ] CIFAR-10 ‚â• 80%
- [ ] Custom O(1) kernel working
- [ ] Diffusion FID < 50 (Track 39)
- [ ] 7 new validation tracks (all)

---

## Publication Targets

Based on results, write 2-3 papers:

| Paper | Venue | Key Finding | Tracks |
|-------|-------|-------------|--------|
| "Spectral Normalization for Stable EqProp" | NeurIPS/ICML | SN is necessary & sufficient | A, F |
| "O(1) Memory Training via Equilibrium" | MLSys | Train deep networks on small GPUs | B |
| "Adaptive Compute Transformers" | ACL/EMNLP | Variable thinking time | C |
| "Energy-Based Uncertainty" | ICLR | Better OOD detection | E |

---

## Getting Started

### Day 1: Verify Foundation

```bash
# 1. Check all existing tracks pass
python verify.py --quick

# 2. Verify baseline CIFAR-10 result
python verify.py --track 33

# 3. Verify Transformer works
python verify.py --track 14
```

### Week 1: Start Track A1

```bash
# Create experiment file
cp experiments/template.py experiments/track_a1_stability.py

# Run ablation study
python experiments/track_a1_stability.py --architecture LoopedMLP --sn True
python experiments/track_a1_stability.py --architecture LoopedMLP --sn False

# Plot L(t) curves
python scripts/plot_lipschitz.py
```

---

## Dependencies & Requirements

### Software

```bash
# requirements.txt
torch>=2.0
torchvision
numpy
scipy
matplotlib
scikit-learn
tqdm
```

### Hardware

- **GPU**: NVIDIA RTX 3060 8GB or equivalent
- **RAM**: 16GB system memory
- **Storage**: 20GB for datasets + checkpoints

### Datasets (auto-download)

- CIFAR-10 (500MB)
- SVHN (2GB)
- CIFAR-100 (500MB)
- WikiText-2 (200MB)
- Shakespeare (1MB)

---

## Notes & Tips

### From Feasibility Analysis

1. **Track B**: Start with gradient checkpointing, defer custom kernel
2. **Track D**: Optional stretch goal, complex implementation
3. **Track A2**: Most critical for publication impact
4. **Statistical rigor**: Always use 3+ seeds, report confidence intervals

### Common Pitfalls

- ‚ùå **Don't**: Train without spectral norm (will diverge)
- ‚ùå **Don't**: Use 30 equilibrium steps for all experiments (5-15 often sufficient)
- ‚ùå **Don't**: Expect SOTA results (goal is to prove viability)
- ‚úÖ **Do**: Track Lipschitz constant throughout training
- ‚úÖ **Do**: Compare to matched-capacity baselines
- ‚úÖ **Do**: Report negative results (e.g., if Track D fails)

### Debugging Tips

```python
# Check if spectral norm is working
model = LoopedMLP(784, 256, 10, use_spectral_norm=True)
L = model.compute_lipschitz()
assert L < 1.1, f"Lipschitz too high: {L}"

# Check if equilibrium converges
h_traj = model(x, return_trajectory=True)[1]
deltas = [(h_traj[i] - h_traj[i-1]).norm() for i in range(1, len(h_traj))]
print(f"Convergence: {deltas[-5:]}")  # Should decrease
```

---

## Questions/Issues

If stuck, check:
1. `FEASIBILITY.md` - Risk mitigation strategies
2. `README.md` - Architecture details
3. `validation/tracks/` - Reference implementations
4. Existing verification results: `results/verification_notebook.md`

**Contact**: See existing track implementations for working examples
