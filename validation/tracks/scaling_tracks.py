
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import sys
from pathlib import Path
from ..notebook import TrackResult
from ..utils import create_synthetic_dataset, train_model, evaluate_accuracy

# Enhance import path
root_path = Path(__file__).parent.parent.parent
if str(root_path) not in sys.path:
    sys.path.append(str(root_path))

from models import (
    LoopedMLP, NeuralCube, LazyEqProp
)

def track_5_neural_cube(verifier) -> TrackResult:
    """Track 3 (README): 3D Neural Cube with local connectivity."""
    print("\n" + "="*60)
    print("TRACK 5: Neural Cube 3D Topology")
    print("="*60)
    
    start = time.time()
    cube_size = 6
    input_dim, output_dim = 64, 10
    
    X, y = create_synthetic_dataset(verifier.n_samples, input_dim, 10, verifier.seed)
    
    print(f"\n[5a] Training {cube_size}×{cube_size}×{cube_size} Neural Cube...")
    cube = NeuralCube(cube_size=cube_size, input_dim=input_dim, output_dim=output_dim)
    
    topo = cube.get_topology_stats()
    train_model(cube, X, y, epochs=verifier.epochs, lr=0.01, name="3D Cube")
    acc = evaluate_accuracy(cube, X, y)
    
    print(f"\n  Neurons: {topo['n_neurons']}")
    print(f"  Connection reduction: {topo['connection_reduction']*100:.1f}%")
    print(f"  Accuracy: {acc*100:.1f}%")
    
    # Visualize
    with torch.no_grad():
        _, traj = cube(X[:1], return_trajectory=True)
        viz = cube.visualize_cube_ascii(traj[-1])
    
    score = min(100, acc * 100) if acc > 0.5 else 30
    status = "pass" if score >= 80 else ("partial" if score >= 50 else "fail")
    
    evidence = f"""
**Claim**: 3D lattice topology with 26-neighbor connectivity achieves equivalent learning with 91% fewer connections.

**Experiment**: Train 6×6×6 Neural Cube on classification task.

| Property | Value |
|----------|-------|
| Cube Dimensions | {cube_size}×{cube_size}×{cube_size} |
| Total Neurons | {topo['n_neurons']} |
| Local Connections | {topo['local_connections']} |
| Fully-Connected Equiv. | {topo['fully_connected_equivalent']} |
| **Connection Reduction** | **{topo['connection_reduction']*100:.1f}%** |
| Final Accuracy | {acc*100:.1f}% |

**3D Visualization** (z-slices):
```
{viz}
```

**Biological Relevance**: Maps to cortical microcolumns; enables neurogenesis/pruning.
"""
    
    improvements = []
    if acc < 0.9:
        improvements.append(f"Accuracy {acc*100:.0f}% below expectations; tune hyperparameters")
    
    return TrackResult(
        track_id=5, name="Neural Cube 3D Topology",
        status=status, score=score,
        metrics={"accuracy": acc, "connection_reduction": topo['connection_reduction']},
        evidence=evidence,
        time_seconds=time.time() - start,
        improvements=improvements
    )

def track_10_memory_scaling(verifier) -> TrackResult:
    """Scaling: O(1) memory with depth."""
    print("\n" + "="*60)
    print("TRACK 10: O(1) Memory Scaling")
    print("="*60)
    
    start = time.time()
    input_dim, hidden_dim, output_dim = 64, 64, 10
    depths = [10, 25, 50, 100] if not verifier.quick_mode else [10, 25, 50]
    
    print("\n[10a] Measuring memory vs depth...")
    results = {}
    
    for depth in depths:
        model = LoopedMLP(input_dim, hidden_dim, output_dim, 
                        use_spectral_norm=True, max_steps=depth)
        
        # Compute theoretical memory
        param_mem = sum(p.numel() * 4 for p in model.parameters()) / 1e6
        eqprop_act_mem = 32 * hidden_dim * 4 / 1e6  # O(1)
        bp_act_mem = 32 * hidden_dim * depth * 4 / 1e6  # O(n)
        
        results[depth] = {
            'eqprop': param_mem + eqprop_act_mem,
            'backprop': param_mem + bp_act_mem,
            'ratio': (param_mem + bp_act_mem) / (param_mem + eqprop_act_mem)
        }
        
        print(f"  Depth {depth:3d}: EqProp={results[depth]['eqprop']:.2f}MB, "
              f"Backprop={results[depth]['backprop']:.2f}MB, "
              f"Ratio={results[depth]['ratio']:.1f}×")
    
    max_ratio = max(r['ratio'] for r in results.values())
    score = min(100, max_ratio * 10)
    status = "pass" if max_ratio > 5 else ("partial" if max_ratio > 2 else "fail")
    
    table = "\n".join([
        f"| {d} | {r['eqprop']:.2f} MB | {r['backprop']:.2f} MB | {r['ratio']:.1f}× |"
        for d, r in results.items()
    ])
    
    evidence = f"""
**Claim**: EqProp requires O(1) memory (constant with depth), Backprop requires O(n).

**Experiment**: Measure theoretical memory usage at varying depths.

| Depth | EqProp | Backprop | Savings |
|-------|--------|----------|---------|
{table}

**Key Finding**: At depth {depths[-1]}, EqProp uses **{results[depths[-1]]['ratio']:.1f}× less memory**.

**Why**: EqProp only stores current state; Backprop stores all intermediate activations.
"""
    
    return TrackResult(
        track_id=10, name="O(1) Memory Scaling",
        status=status, score=score,
        metrics={"results": results, "max_ratio": max_ratio},
        evidence=evidence,
        time_seconds=time.time() - start,
        improvements=[]
    )

def track_11_deep_network(verifier) -> TrackResult:
    """Scaling: 100-layer network with gradient flow."""
    print("\n" + "="*60)
    print("TRACK 11: Deep Network (100 layers)")
    print("="*60)
    
    start = time.time()
    
    # Create deep model
    depth = 50 if verifier.quick_mode else 100
    input_dim, hidden_dim, output_dim = 64, 64, 10
    
    print(f"\n[11a] Creating {depth}-step model...")
    model = LoopedMLP(input_dim, hidden_dim, output_dim, 
                     use_spectral_norm=True, max_steps=depth)
    
    X, y = create_synthetic_dataset(verifier.n_samples, input_dim, 10, verifier.seed)
    
    print(f"[11b] Training...")
    losses = train_model(model, X, y, epochs=verifier.epochs, name=f"{depth}-deep")
    acc = evaluate_accuracy(model, X, y)
    
    # Check gradient flow
    model.eval()
    x = X[:1]
    with torch.enable_grad():
        out, trajectory = model(x, return_trajectory=True)
        loss = F.cross_entropy(out, y[:1])
        loss.backward()
    
    # Check if gradients reached all layers (via input gradient)
    # Spectral norm makes .weight a computed tensor; we need the original parameter
    if hasattr(model.W_in, 'parametrizations'):
        w_param = model.W_in.parametrizations.weight.original
    else:
        w_param = model.W_in.weight
        
    grad_exists = w_param.grad is not None
    grad_mag = w_param.grad.abs().mean().item() if grad_exists else 0
    
    # Key claim: credit assignment through deep networks - accuracy is primary metric
    score = min(100, acc * 100) if acc > 0.5 else 30
    status = "pass" if acc > 0.9 else ("partial" if acc > 0.5 else "fail")
    
    evidence = f"""
**Claim**: EqProp enables credit assignment through 100+ effective layers.

**Experiment**: Train {depth}-step LoopedMLP (equivalent to {depth}-layer network).

| Metric | Value |
|--------|-------|
| Effective Depth | {depth} layers |
| Final Accuracy | {acc*100:.1f}% |
| Gradient Flow | {"✅ Present" if grad_exists else "❌ Missing"} |
| Input Gradient Magnitude | {grad_mag:.6f} |

**Key Finding**: Spectral normalization enables stable gradient propagation through {depth} layers.
"""
    
    improvements = []
    if acc < 0.9:
        improvements.append("Accuracy below expectations; may need more epochs")
    if grad_mag < 1e-6:
        improvements.append("Very small gradients; check for vanishing gradient issue")
    
    return TrackResult(
        track_id=11, name="Deep Network (100 layers)",
        status=status, score=score,
        metrics={"depth": depth, "accuracy": acc, "grad_magnitude": grad_mag},
        evidence=evidence,
        time_seconds=time.time() - start,
        improvements=improvements
    )

def track_12_lazy_updates(verifier) -> TrackResult:
    """Scaling: Lazy/Event-driven updates for FLOP savings."""
    print("\n" + "="*60)
    print("TRACK 12: Lazy Event-Driven Updates")
    print("="*60)
    
    start = time.time()
    input_dim, hidden_dim, output_dim = 64, 128, 10
    
    X_train, y_train = create_synthetic_dataset(verifier.n_samples, input_dim, 10, verifier.seed)
    X_test, y_test = create_synthetic_dataset(verifier.n_samples//5, input_dim, 10, verifier.seed+1)
    
    # Test different epsilon thresholds
    epsilons = [0.001, 0.01, 0.1]
    results = {}
    
    # First, train standard model for accuracy baseline
    print("\n[12a] Training standard EqProp (baseline)...")
    baseline = LoopedMLP(input_dim, hidden_dim, output_dim, use_spectral_norm=True)
    train_model(baseline, X_train, y_train, epochs=verifier.epochs, lr=0.01, name="Standard")
    baseline_acc = evaluate_accuracy(baseline, X_test, y_test)
    print(f"  Baseline accuracy: {baseline_acc*100:.1f}%")
    
    print("\n[12b] Testing lazy models with different thresholds...")
    for eps in epsilons:
        model = LazyEqProp(input_dim, hidden_dim, output_dim, epsilon=eps, use_spectral_norm=True)
        train_model(model, X_train, y_train, epochs=verifier.epochs, lr=0.01, name=f"ε={eps}")
        
        # Measure accuracy
        acc = evaluate_accuracy(model, X_test, y_test)
        
        # Measure FLOP savings on a forward pass
        model.stats.reset()
        with torch.no_grad():
            _ = model(X_test, steps=30)
        savings = model.get_flop_savings()
        
        results[eps] = {
            'accuracy': acc,
            'flop_savings': savings,
            'acc_gap': baseline_acc - acc,
        }
        
        print(f"  ε={eps}: acc={acc*100:.1f}% | savings={savings:.1f}%")
    
    # Best result: highest savings with minimal acc loss
    best_eps = max(results.keys(), key=lambda e: results[e]['flop_savings'] - results[e]['acc_gap'] * 10)
    best = results[best_eps]
    
    # Evaluate
    high_savings = best['flop_savings'] > 50
    low_acc_loss = best['acc_gap'] < 0.1
    
    if high_savings and low_acc_loss:
        score = 100
        status = "pass"
    elif high_savings or low_acc_loss:
        score = 70
        status = "partial"
    else:
        score = 40
        status = "fail"
    
    table = "\n".join([
        f"| {eps} | {r['accuracy']*100:.1f}% | {r['flop_savings']:.1f}% | {r['acc_gap']*100:+.1f}% |"
        for eps, r in results.items()
    ])
    
    evidence = f"""
**Claim**: Event-driven updates achieve massive FLOP savings by skipping inactive neurons.

**Experiment**: Train LazyEqProp with different activity thresholds (ε).

| Baseline | Accuracy |
|----------|----------|
| Standard EqProp | {baseline_acc*100:.1f}% |

| Threshold (ε) | Accuracy | FLOP Savings | Acc Gap |
|---------------|----------|--------------|---------|
{table}

**Best Configuration**: ε={best_eps}
- FLOP Savings: {best['flop_savings']:.1f}%
- Accuracy Gap: {best['acc_gap']*100:+.1f}%

**How It Works**:
1. Track input change magnitude per neuron per step
2. Skip update if |Δinput| < ε
3. Inactive neurons keep previous state

**Hardware Impact**: Enables event-driven neuromorphic chips with massive energy savings.
"""
    
    improvements = []
    if not high_savings:
        improvements.append(f"FLOP savings {best['flop_savings']:.0f}% below 50% target; lower epsilon")
    if not low_acc_loss:
        improvements.append(f"Accuracy gap {best['acc_gap']*100:.1f}% too large; reduce epsilon")
    
    return TrackResult(
        track_id=12, name="Lazy Event-Driven Updates",
        status=status, score=score,
        metrics={"best_eps": best_eps, "results": results},
        evidence=evidence,
        time_seconds=time.time() - start,
        improvements=improvements
    )
